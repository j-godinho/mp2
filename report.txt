MP2

Grupo 20:
Bernardo Casaleiro 	nº87827
João Godinho 		nº87830

Bibliotecas externas usadas:

	- nltk (usado para retirar os tokens dos ficheiros e usado para obter as stopwords da língua Portuguesa e sinais de pontuação)

	- glob (usado para ler todos os ficheiros de uma diretoria)

	- scikit-learn (usado para calcular n-grams e TF-IDF)

Opções Tomadas:
	-A linguagem usada foi Python, pela sua simplicidade e existência de  grande quantidade de bibliotecas referentes a este assunto.

	-Todo o programa pode ser corrido através do script run.sh.
		# Este script é responsável por criar já as diretorias onde os ficheiros de treino e de teste irão ser escritos após serem
			normalizados, assim como os ficheiros de unigramas e bigramas referentes a cada autor, com e sem alisamento.

		# O script contém duas flags, uma de normalização, que pode ser escolhida para fazer 4 tipos de normalização e outra chamada
			de exp_flag usada para escolher qual exercicio pretendemos correr:

			Norm_flag:

			Flag = 0	-	A pontuação é rodeada por espaços brancos (pedido na alinea a) do enunciado)

			Flag = 1	-	Texto é passado para letra minúscula

			Flag = 2	-	Remoção da pontuação e case-sensitive

			Flag = 3	-	Stop-Words Portuguesas removidas

	- A função word_tokenize da biblioteca nltk auxiliou-nos bastante no parsing dos tokens dado que já faz automaticamente a remoção
	de tokens muito específicos, como as reticências e não remove hífens dentro de palavras, por exemplo "come-as", mantendo toda a palavra como um token.

		# Os ficheiros de output seguem a estrutura do ficheiro 'bigramasDEMO.txt' fornecido pelo professor.

		# O script gera para cada autor, agrupando todos os seus ficheiros de treino, 4 ficheiros:
			- contagem de unigramas
			- contagem de bigramas
			- contagem de unigramas com alisamento usando add-one
			- contagem de bigramas com alisamento usando add-one
			* Volto a salientar que nestes ficheiros se encontram as contagens com alisamento e não as probabilidades,
			para o calculo da sua probabilidade basta recorrer à formula P(wi) = ci*/N para os unigramas e
			P(wi|wi-1) = ci* / (#wi-1) para os bigramas.

	- Para retirar os n-grams foi usada a classe CountVerifier da biblioteca scikit-learn.
		Para uma das experiências não quisemos que a pontuação fosse ignorada portanto tivemos que utilizar um
		tokenizer que não é o default da biblioteca. Por default esta usa os sinais de pontuação como separadores.

	- A função word_tokenize da biblioteca nltk auxiliou-nos bastante no parsing dos tokens dado que já faz
		automaticamente a remoção de tokens muito específicos, como as reticências e outros símbolos não
		removendo hifenes dentro de palavras, como por exemplo "come-as", mantendo toda a palavra como um token.

Experiências:

	Apesar de termos desenvolvido várias técnicas de normalização, escolhemos apenas usar para as experiências as que achamos mais pertinentes para comentar.

	1ª: A primeira experiencia e mais básica limita-se a fazer as contagens e calculo de probabilidade de
		unigramas ou bigramas dos ficheiros de treino de cada autor. De seguida é feita a divisão para cada ficheiro
		de teste em unigramas ou bigramas, respetivamente, e calculada a probabilidade desse ficheiro de teste
		pertencer a cada autor. Este calculo é feito através da multiplicação das probabilidades de cada unigrama
		ou bigrama ocorrentes no ficheiro de teste e calculadas usando ficheiros de treino. Para isto foram usadas
		duas versões, uma com alisamento add-one e outra sem alisamento.

	2ª: Stop Words

	3ª: Esta experiencia é uma experiencia mais conceptual. Nela vamos calcular o score tf-idf (Term Frequency - Inverse Document Frequency)
		para cada n-grama dos ficheiros de treino e é obtido uma lista com uma percentagem das palavras mais relevantes do autor.
		O mesmo processo é aplicado para os ficheiro de teste e calculado a precisão, abrangência e F-Measure entre cada autor e o teste.
		Sendo o autor escolhido consuante a medida F-Measure.

	4ª: Uma das experiências que fizemos foi calcular a média de palavras por frase de cada escritor.
		Para calcularmos o número de frases contámos simplesmente o número pontos de exclamação e interrogação,
		pontos finais, reticências e ponto e vírgula [!?.(...);]. O que nos pareceu uma boa aproximação para contar as frases.
		Para cada escritor foi calculado a média de palavras por frase e para cada caso de teste foi feito o
		mesmo procedimento, comparando-se depois a mínima diferença entre cada escritor e aquele caso de teste.


Observações:

	Todas as técnicas de normalização foram testadas e concluímos que:

		- Apesar de acharmos que usando um texto case sensitive traria melhores resultados (Rosa != rosa), verificámos que não se notou grande diferença.

		- O mesmo se sucedeu com o uso de sinais de pontuação para os cálculos dos n-gramas.


	Em relação às experiências:

	1ª: A nossa espetativa para esta experiencia era alta, no entanto ao depararmo-nos com os resultados podemos afirmar que é bastante incompleta.
		As probabilidades são muito próximas exceto em casos muito específicos que atribuímos possivelmente a vocabulário e estilos de escrita que se
		distingue dos restantes autores significativamente.

	2ª:

	3ª:

	4ª: Apesar de no início acharmos que obteríamos bons resultados nesta experiência facilmente verificámos que os casos de teste eram demasiado
	pequenos para ter uma boa precisão. No entanto para escritores como José Saramago	saberíamos que facilmente este escritor iria ter uma grande
	média comparativamente a outros, e foi isso que podemos confirmar. No entanto para escritores com uma escrita mais comum dificilmente conseguimos
	tirar conclusões acerca do escritor do caso de teste.


Resultados:
