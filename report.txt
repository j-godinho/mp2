MP2

Grupo 20:
Bernardo Casaleiro 	nº87827
João Godinho 		nº87830

Bibliotecas externas usadas:

	- nltk (usado para retirar os tokens dos ficheiros e usado para obter as stopwords da língua Portuguesa e sinais de pontuação)

	- scikit-learn (usado para calcular n-grams e TF-IDF)

Opções Tomadas:
	-A linguagem usada foi Python, pela sua simplicidade e existência de  grande quantidade de bibliotecas referentes a este assunto.

	-Todo o programa pode ser corrido através do script run.sh.
		# Este script é responsável por criar já as diretorias onde os ficheiros de treino e de teste irão ser escritos após serem
			normalizados, assim como os ficheiros de unigramas e bigramas referentes a cada autor, com e sem alisamento.

		# O script contém duas flags, uma de normalização, que pode ser escolhida para fazer 4 tipos de normalização e outra chamada
			de exp_flag usada para escolher qual exercício pretendemos correr:

			Norm_flag:

			Flag = 0	-	A pontuação é rodeada por espaços brancos (pedido na alínea a) do enunciado)

			Flag = 1	-	Texto é passado para letra minúscula e pontuação é ignorada (mantém-se na normalização mas ignorado na tokenization)

			Flag = 2	-	Remoção da pontuação e case-sensitive

			Flag = 3	-	Stop-Words Portuguesas removidas e pontuação ignorada

	- A função word_tokenize da biblioteca nltk auxiliou-nos bastante no parsing dos tokens dado que já faz automaticamente a remoção
	  de tokens muito específicos, como as reticências e não remove hífenes dentro de palavras, por exemplo "come-as", mantendo toda a palavra como um token.
		Tem também outras características que facilitam bastante esta tarefa.

		# Os ficheiros de output seguem a estrutura do ficheiro 'bigramasDEMO.txt' fornecido pelo professor.

		# O script gera para cada autor, agrupando todos os seus ficheiros de treino, 4 ficheiros:
			- contagem de unigramas
			- contagem de bigramas
			- contagem de unigramas com alisamento usando add-one
			- contagem de bigramas com alisamento usando add-one
			* Volto a salientar que nestes ficheiros se encontram as contagens com alisamento e não as probabilidades,
			para o cálculo da sua probabilidade basta recorrer à formula P(wi) = ci*/N para os unigramas e
			P(wi|wi-1) = ci* / (#wi-1) para os bigramas.

	- Para retirar os n-grams foi usada a classe CountVerifier da biblioteca scikit-learn.
		Para a alínea a) não quisemos que a pontuação fosse ignorada tivemos então que utilizar um
		tokenizer que não é o default da biblioteca. Por default esta usa os sinais de pontuação como separadores.

	- A função word_tokenize da biblioteca nltk auxiliou-nos bastante no parsing dos tokens dado que já faz
		automaticamente a remoção de tokens muito específicos, como as reticências e outros símbolos não
		removendo hífenes dentro de palavras, como por exemplo "come-as", mantendo toda a palavra como um token.

Experiências:

	Apesar de termos desenvolvido várias técnicas de normalização, escolhemos apenas usar para as experiências as que achamos mais pertinentes para comentar:

	1ª: A primeira experiencia e mais básica limita-se a fazer as contagens e calculo de probabilidade de
		unigramas ou bigramas dos ficheiros de treino de cada autor. De seguida é feita a divisão para cada ficheiro
		de teste em unigramas ou bigramas, respetivamente, e calculada a probabilidade desse ficheiro de teste
		pertencer a cada autor. Este calculo é feito através da multiplicação das probabilidades de cada unigrama
		ou bigrama ocorrentes no ficheiro de teste e calculadas usando ficheiros de treino. Para isto foram usadas
		duas versões, uma com alisamento add-one e outra sem alisamento.

	2ª: Após obtermos os resultados da primeira experiência verificámos que muitas das palavras eram repetidas, com uma pequena pesquisa verificámos que
		muitas delas eram consideradas "Stop Words". Portanto esta experiência foi baseada na remoção das Stop Words Portuguesas obtidas através
		da biblioteca nltk. Reparou-se também que o uso de pontuação para os n-gramas não trouxe grandes melhorias portanto foram ignorados os sinais de
		pontuação ao recolher os tokens. Esta experiência será equivalente à experiência 1, com a diferença da remoção da pontuação e dos tokens que pertencem
		às stop words da Língua Portuguesa.

	3ª: Esta experiencia é uma experiencia mais conceptual e a mais complexa. Nela vamos calcular o score tf-idf (Term Frequency - Inverse Document Frequency)
		para cada n-grama dos ficheiros de treino e é obtido uma lista com uma percentagem das palavras mais relevantes do autor.
		O mesmo processo é aplicado para os ficheiro de teste e calculado a precisão, abrangência e F-Measure entre cada autor e o teste.
		Sendo o autor escolhido consuante a medida F-Measure.

	4ª: Por último quisemos tomar uma abordagem muito simples e calcular a média de palavras por frase de cada escritor.
		Para calcularmos o número de frases contámos o número pontos de exclamação e interrogação,
		pontos finais, reticências e ponto e vírgula [!?.(...);]. O que nos pareceu uma boa aproximação para contar as frases.
		Para cada escritor foi calculado a média de palavras por frase e para cada caso de teste foi feito o
		mesmo procedimento, comparando-se depois a mínima diferença entre cada escritor e aquele caso de teste.


Observações:
	Os resultados das experiências encontram-se no ficheiro "results.txt"

	Todas as técnicas de normalização foram testadas e concluímos que:

		- Apesar de acharmos que usando um texto case sensitive traria melhores resultados (Rosa != rosa), verificámos que não se notou grande diferença.

		- O mesmo se sucedeu com o uso de sinais de pontuação para os cálculos dos n-gramas, portanto em todas as experiências à exceção da alínea a) e da
		última experiência.


	Em relação às experiências:

	1ª: A nossa expectativa para esta experiencia era alta, no entanto ao depararmo-nos com os resultados podemos afirmar que é bastante incompleta.
		As probabilidades são muito próximas exceto em casos muito específicos que atribuímos possivelmente a vocabulário e estilos de escrita que se
		distingue dos restantes autores significativamente. Podemos dizer que a dimensão do corpus ainda era muito reduzida para o uso de bigramas.

	2ª: Como pudemos observar pelos resultados obtidos, a remoção das stop words não trouxe grande diferença à experiência 1, variando apenas num caso
		de teste. Podemos concluir que a remoção das stop words é um bom método para filtrar grandes quantidades de texto com o objetivo da redução
		do seu tamanho. Porque para os mesmos casos de teste, foram obtidos resultados muito semelhantes, verificando-se que não houve perda de informação
		vital do texto.

	3ª: Esta experiencia é bastante interessante para o caso do escritor escrever sobre os mesmos temas tanto nos ficheiros de treino como no ficheiro
		de teste, no entanto não foi o que se verificou pois os temas eram variados. Podemos assim concluir que não é a melhor abordagem para um problema
		deste género.

	4ª: Apesar de no início acharmos que obteríamos bons resultados nesta experiência facilmente verificámos que os casos de teste eram demasiado
	pequenos para ter uma boa precisão. No entanto para escritores como José Saramago saberíamos que facilmente este escritor iria obter uma grande
	média de palavras por frase comparativamente a outros, e foi isso que pudemos confirmar. No entanto para escritores com uma escrita mais comum dificilmente conseguimos tirar conclusões acerca do escritor do caso de teste.
		Após recolher os dados reparámos que podíamos usar este método apenas para identificar escritores que realmente tenham uma escrita fora do normal em termos
		de uso de pontuação, como por exemplo para reconhecer poemas vs prosa. Podíamos melhorar esta abordagem fazendo com que o nosso identificador apenas
		identificasse o escritor se este tivesse uma diferença mínima maior do que um determinado valor, evitando assim que se identificasse um escritor quando
		existisse outro que tivesse uma média de palavras também parecida.
